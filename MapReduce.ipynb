{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling spark session to register application\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MapReduce\") \\\n",
    "    .getOrCreate()\n",
    "# lambda word: (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can set number of partitions by changing local value. \n",
    "\n",
    "Spark can run 1 concurrent task for every partition of an RDD (up to the number of cores in the cluster).\n",
    "If you’re cluster has 20 cores, you should have at least 20 partitions (in practice 2–3x times more). \n",
    "From the other hand a single partition typically shouldn’t contain more than 128MB and a single \n",
    "shuffle block cannot be larger than 2GB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want change property of SparkContext or want to create new SparkContext, first you have to stop existing one.\n",
    "# sc.stop() stop existing SparkContext\n",
    "# sc = SparkContext(\"locals[3]\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default Partitions\n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content of demo.txt \n",
    "'''\n",
    "['Matrix', 'factorization', 'works', 'great', 'for', 'building', \n",
    "'recommender', 'systems.', 'I', 'think', 'it', 'got', 'pretty', \n",
    "'popular', 'after', 'the', 'Netflix', 'prize', 'competition.', \n",
    "'All', 'you', 'need', 'to', 'build', 'one', 'is', 'information', \n",
    "'about', 'which', 'user', 'bought', 'or', 'rated', 'which', 'items',\n",
    "'and', 'youre', 'good', 'to', 'go.', 'And', 'I', 'was', 'surprised',\n",
    "'how', 'amazingly', 'simple', 'to', 'build', 'one', 'with', 'Pyspark', \n",
    "'ML', 'libraries.', 'So', \"I'll\", 'demonstrate', 'how', 'to', 'code', \n",
    "'one', 'up', 'quickly', 'using', 'RDDs', 'and', 'DataFrames', 'separately.']\n",
    "'''\n",
    "text_file = sc.textFile(\"check.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1386/1*fs9qiYrqphSuFuKKVkEmaQ.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define punctuation\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "# Converting lines in words using flatMap\n",
    "flat_map_result = text_file.flatMap(lambda line: line.split(\" \")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[3] at readRDDFromFile at PythonRDD.scala:247"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting a list of words from flat map result\n",
    "words = flat_map_result.collect()\n",
    "#distribute input to available partitions\n",
    "words_rdd = sc.parallelize(words)\n",
    "words_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 8\n",
      "\n",
      "Partition - 1\n",
      "\n",
      "Partition - 2\n",
      "\n",
      "Partition - 3\n",
      "\n",
      "Partition - 4\n",
      "\n",
      "Partition - 5\n",
      "\n",
      "Partition - 6\n",
      "\n",
      "Partition - 7\n",
      "\n",
      "Partition - 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions: {}\".format(words_rdd.getNumPartitions()))\n",
    "# Collecting different partitions in the list \n",
    "partitions = words_rdd.glom().collect()\n",
    "\n",
    "print()\n",
    "count = 1\n",
    "# print list of words every partition contain\n",
    "for ix,partition in enumerate(partitions):\n",
    "    print(\"Partition -\",count)\n",
    "    #print(partition)\n",
    "    print()\n",
    "    for ind,word in enumerate(partition):\n",
    "        for i in word:\n",
    "            if i in punctuations: \n",
    "                #partition[ind] = word.translate(i, \"\").strip()\n",
    "                word=word[:-1]\n",
    "                partitions[ix][ind]=word\n",
    "    #print(len(word))\n",
    "    #print(partition)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Phase\n",
    "\n",
    "Map phase runs in parallel and (if possible) locally on each data block. Instead of delivering terabytes of data to a program, a small, user–defined program is copied onto data servers and does everything with them that does not require shuffling and data movement (shuffle);\n",
    "\n",
    "In map phase map fuction will be performed on every rdd. In below case each word in rdd will converted into (word,count)tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 8\n",
      "\n",
      "Partition - 1\n",
      "[]\n",
      "\n",
      "Partition - 2\n",
      "[('world!', 6, 1)]\n",
      "\n",
      "Partition - 3\n",
      "[]\n",
      "\n",
      "Partition - 4\n",
      "[('text.', 5, 1)]\n",
      "\n",
      "Partition - 5\n",
      "[]\n",
      "\n",
      "Partition - 6\n",
      "[('text', 4, 1)]\n",
      "\n",
      "Partition - 7\n",
      "[]\n",
      "\n",
      "Partition - 8\n",
      "[('okay', 4, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "map_words_rdd = words_rdd.map(lambda word: (word,len(word), 1))\n",
    "#wordCounts=words.map(lambda word:(len(word),1)).reduceByKey(lambda a,b:a+b)\n",
    "print(\"Number of partitions: {}\".format(map_words_rdd.getNumPartitions()))\n",
    "# Collecting different partitions in the list \n",
    "partitions = map_words_rdd.glom().collect()\n",
    "\n",
    "print()\n",
    "count = 1\n",
    "# print list of tuple every partition contain\n",
    "for partition in partitions:\n",
    "    print(\"Partition -\",count)\n",
    "    print(partition)\n",
    "    print()\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Phase\n",
    "\n",
    "This phase complements Map with aggregate operations.\n",
    "\n",
    "In the reduce phase, we calculate count value by adding values of the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 8\n",
      "\n",
      "Partition - 1\n",
      "[]\n",
      "\n",
      "Partition - 2\n",
      "[]\n",
      "\n",
      "Partition - 3\n",
      "[]\n",
      "\n",
      "Partition - 4\n",
      "[]\n",
      "\n",
      "Partition - 5\n",
      "[(4, 2)]\n",
      "\n",
      "Partition - 6\n",
      "[(5, 1)]\n",
      "\n",
      "Partition - 7\n",
      "[(6, 1)]\n",
      "\n",
      "Partition - 8\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduce_words_rdd = map_words_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(\"Number of partitions: {}\".format(reduce_words_rdd.getNumPartitions()))\n",
    "# Collecting different partitions in the list \n",
    "partitions = reduce_words_rdd.glom().collect()\n",
    "print()\n",
    "count = 1\n",
    "# print list of tuple every partition contain\n",
    "for partition in partitions:\n",
    "    print(\"Partition -\",count)\n",
    "    print(partition)\n",
    "    print()\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all partitons\n",
    "\n",
    "In this phase all partitions are combine into one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 1\n",
      "\n",
      "Partition - 1\n",
      "[(4, 2), (5, 1), (6, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine all partitions into one partition\n",
    "counts = reduce_words_rdd.coalesce(1)\n",
    "\n",
    "print(\"Number of partitions: {}\".format(counts.getNumPartitions()))\n",
    "partitions = counts.glom().collect()\n",
    "print()\n",
    "count = 1\n",
    "for partition in partitions:\n",
    "    print(\"Partition -\",count)\n",
    "    print(partition)\n",
    "    print()\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
